{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hd/anaconda2/lib/python2.7/site-packages/matplotlib/font_manager.py:273: UserWarning: Matplotlib is building the font cache using fc-list. This may take a moment.\n",
      "  warnings.warn('Matplotlib is building the font cache using fc-list. This may take a moment.')\n"
     ]
    }
   ],
   "source": [
    "import csv as csv \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.optimize import curve_fit\n",
    "from scipy.optimize import minimize\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.pipeline import make_pipeline\n",
    "# %matplotlib outline \n",
    "\n",
    "# Open up the csv file in to a Python object\n",
    "data = pd.read_csv('data.csv', header=0)\n",
    "data = data.reindex(np.random.permutation(data.index))\n",
    "\n",
    "\n",
    "# dataFrame.dtypes\n",
    "# dataFrame.info()\n",
    "# dataFrame.describe()\n",
    "# dataFrame.x.mean()\n",
    "\n",
    "# dataFrame['x'].hist(bins=16, range=(-5,5), alpha = .2)\n",
    "#P.show()\n",
    "# dataFrame.head()\n",
    "# type(dataFrame.x)\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "use only the first 20 data points in your file.\n",
    "Solve the curve fitting regression problem using error function minimisation.\n",
    "use a validation approach to characterise the goodness of fit for polynomials of different order\n",
    "distinguish overfitting,underfitting, and the best fit\n",
    "obtain an estimate for the noise variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "X_train1 = data.iloc[0:80,0]\n",
    "Y_train = data.iloc[0:80,1]\n",
    "Y_train1 = Y_train\n",
    "\n",
    "X_test1 = data.iloc[80:100,0]\n",
    "Y_test = data.iloc[80:100,1]\n",
    "\n",
    "\n",
    "x_plot = np.linspace(-5, 5, 100)\n",
    "# create matrix versions of these arrays\n",
    "X_train = X_train1[:, np.newaxis]\n",
    "X_plot = x_plot[:, np.newaxis]\n",
    "X_test = X_test1[:,np.newaxis]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "low = 3\n",
    "high = 9\n",
    "deg = [5,7,9,11]\n",
    "error = []\n",
    "\n",
    "plt.scatter(X_train, Y_train, label=\"training points\")\n",
    "for degree in deg:\n",
    "    model = make_pipeline(PolynomialFeatures(degree), Ridge())\n",
    "    model.fit(X_train, Y_train)\n",
    "    y_plot = model.predict(X_plot)\n",
    "    \n",
    "    #calculate the error on test data\n",
    "    Y_predicted = model.predict(X_test)\n",
    "    err = np.sum((Y_predicted - Y_test)**2)/20\n",
    "    error.append(err)\n",
    "    \n",
    "#     print(model.get_params)\n",
    "    \n",
    "    plt.plot(x_plot, y_plot, label=\"degree %d\" % degree)\n",
    "\n",
    "plt.legend(loc='lower left')\n",
    "\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Error vs degree\n",
    "b = deg\n",
    "plt.scatter(b, error)\n",
    "plt.plot(b, error, label=\"Error vs degree\")\n",
    "plt.legend(loc='lower left')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#tune the parameter alpha\n",
    "#degree = 5\n",
    "alfas = [0,0.1,1,5,10,20,30,100,10000,1000000000000,100000000000000000]\n",
    "error = []\n",
    "for alfa in alfas:\n",
    "    model = make_pipeline(PolynomialFeatures(5), Ridge(alfa))\n",
    "    model.fit(X_train, Y_train1)\n",
    "    y_plot = model.predict(X_plot)\n",
    "    \n",
    "    #calculate the error on test data\n",
    "    Y_predicted = model.predict(X_test)\n",
    "    err = np.sum((Y_predicted - Y_test)**2)/20\n",
    "    error.append(err)\n",
    "    \n",
    "    \n",
    "    plt.plot(x_plot, y_plot, label=\"alpha %d\" % alfa)\n",
    "\n",
    "plt.legend(loc='lower left')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Error vs alpha\n",
    "import math\n",
    "b = alfas\n",
    "plt.scatter(b, (error))\n",
    "plt.plot(b, error, label=\"Error vs alpha\")\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('variance = ', 64956.12673528609)\n"
     ]
    }
   ],
   "source": [
    "#Finding Variance\n",
    "#use degree = 5, alpha = 10\n",
    "from sklearn.linear_model import Lasso\n",
    "data = pd.read_csv('data.csv', header=0)\n",
    "X,Y = data.x, data.y\n",
    "X = X[:, np.newaxis]\n",
    "model = make_pipeline(PolynomialFeatures(7), Lasso(1000))\n",
    "model.fit(X, Y)\n",
    "Y_predicted = model.predict(X)\n",
    "\n",
    "G = (Y_predicted - Y)**2\n",
    "err = np.sum(G)/100\n",
    "\n",
    "print(\"variance = \",err)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#part 1, no regularization\n",
    "\n",
    "z = np.polyfit(X_train, Y_train,3)\n",
    "print(z)\n",
    "hypothesis = np.poly1d(z)\n",
    "\n",
    "y_new1 = hypothesis(X_train)\n",
    " \n",
    "\n",
    "x_new = np.arange(-5.1, -3.0, 0.1)\n",
    "y_new = hypothesis(x_new)\n",
    "\n",
    "plt.plot(X_train,Y_train,'o', x_new, y_new)\n",
    "plt.show()\n",
    "\n",
    "error = 0;\n",
    "for i in range(0,20): \n",
    "        error += ((y_new1 - Y_train)**2).sum()\n",
    "\n",
    "print((error/20)**0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#code for univariate regression\n",
    "\n",
    "X,Y = data.x, data.y\n",
    "#no. of observations\n",
    "m = Y.size\n",
    "\n",
    "\n",
    "\n",
    "def h(theta,x):\n",
    "    '''theta is a column vector, x is a real no\n",
    "    returns (theta.T)(X)'''\n",
    "    result = 0\n",
    "    for i in range(theta.size):\n",
    "            result += theta[i]*(x**i)\n",
    "    return result        \n",
    "            \n",
    "\n",
    "\n",
    "def cost(y,x,h,theta,lamda):\n",
    "    #y is the target data, x is the input data, h is the hypothesis, theta is the parameter vector\n",
    "    m = y.size\n",
    "    error = 0;\n",
    "    predictions = h(theta,X)\n",
    "    #print(predictions)\n",
    "    sqErrors = (predictions - y) ** 2\n",
    "    J =  sqErrors.sum() + lamda*np.square(theta).sum()\n",
    "    J /= m\n",
    "    return J**(0.5)\n",
    "\n",
    "def optimize(w):\n",
    "    return cost(Y,X,h,w,lamda)\n",
    "\n",
    "\n",
    "#theta[i] = coeff of X**i\n",
    "#theta is the initial guess\n",
    "degree = 5;\n",
    "theta = np.ones(shape=(1+degree,1))\n",
    "#set the regularization parameter\n",
    "lamda =  10\n",
    "\n",
    "print(cost(Y,X,h,theta,lamda))\n",
    "theta_new = minimize(optimize,theta)\n",
    "print(cost(Y,X,h,theta_new.x,lamda))\n",
    "\n",
    "Y_predicted = h(theta_new.x,X)\n",
    "\n",
    "# plt.plot(X, Y_predicted,color=\"red\",label=\"Predicted\")\n",
    "# plt.plot(X,Y,color=\"blue\",label=\"Original\")\n",
    "# plt.legend(loc='upper left')\n",
    "# plt.grid(True)\n",
    "# plt.show()\n",
    "print(theta_new.x)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sigma = (((Y_predicted-Y)**2).sum())/100\n",
    "print(sigma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "costArray = np.zeros(shape=(10,10))\n",
    "def regress(deg, l):\n",
    "    \n",
    "    lamda = l\n",
    "    theta_init = np.ones(shape=(1+deg,1))\n",
    "    theta_new = minimize(optimize,theta_init)\n",
    "    return cost(Y_,X,h,theta_new.x,lamda)\n",
    "\n",
    "for i in range(1,11):\n",
    "    for j in range(0,10):\n",
    "        costArray[i-1,j] = regress(i,j*3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(costArray)\n",
    "\n",
    "# from mpl_toolkits.mplot3d import Axes3D\n",
    "# Xaxis = range(0,10)\n",
    "\n",
    "\n",
    "# fig = plt.figure()\n",
    "# ax = fig.add_subplot(111, projection='3d')\n",
    "# ax.scatter(Xaxis, Yaxis, costArray[:,0], c='r', marker='o')\n",
    "# ax.set_xlabel('X Label')\n",
    "# ax.set_ylabel('Y Label')\n",
    "# ax.set_zlabel('Z Label')\n",
    "\n",
    "# b = costArray[5,:]\n",
    "# print(b)\n",
    "\n",
    "# plt.plot(Xaxis,b)\n",
    "# plt.show()\n",
    "\n",
    "np.where(costArray == costArray.min())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
