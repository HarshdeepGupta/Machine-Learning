{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import csv as csv \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import preprocessing\n",
    "from sklearn import svm\n",
    "from sklearn import cross_validation\n",
    "from scipy import linalg\n",
    "\n",
    "# Open up the csv file in to a Python object\n",
    "data = pd.read_csv('2013MT60597.csv',header = -1)\n",
    "# data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 59.2730931   58.37051386  57.85374196  57.35637875  57.19679766\n",
      "  57.06698198  56.27697423  56.16808756  56.11975154  55.40697927\n",
      "  55.25683585  55.02877665  54.98227379  54.53023193  54.14857773\n",
      "  54.09110117  53.54706172  53.28792843  52.7684147   52.4185597\n",
      "  52.16889704  51.74522114  51.33870103  51.17326139  50.44524561]\n"
     ]
    }
   ],
   "source": [
    "X_train_raw = data.iloc[:,0:25]\n",
    "Y_train = data.iloc[:,25]\n",
    "\n",
    "\n",
    "#normalize the data\n",
    "scaler = preprocessing.StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train_raw)\n",
    "\n",
    "a,b,c = linalg.svd(X_train)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Observations:\n",
    "* From the above SVD of the design matrix, we can see that all of the singular values are significant\n",
    "* There is no sudden drop in singular values, which shows that all dimensions hold some significant variance of the data\n",
    "* Even if we take the first 10 features, we wont get better performance\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 44.71217765  44.03878445  39.60494411  37.64408858  35.37886609\n",
      "  33.87416604  28.8948027   28.62889521  28.36832229  27.70021537\n",
      "  26.22306144  24.80111528  22.74054369  22.35806348  21.1204619\n",
      "  19.83268669  18.00740132  17.26323514  14.84546306  13.73647287\n",
      "  13.24691147  11.9778114   11.29207202  10.31837048   8.96458364]\n"
     ]
    }
   ],
   "source": [
    "#Create a subset of the database \n",
    "# choose only those classes whose labels are 2 and 3\n",
    "B = np.zeros(Y_train.shape[0],dtype=bool)\n",
    "for i in range(0,Y_train.shape[0]):\n",
    "    if(Y_train[i] == 2 or Y_train[i] == 3):\n",
    "        B[i] = True\n",
    "\n",
    "X_23 = X_train[B]\n",
    "Y_23 = Y_train[B]\n",
    "# X_23.iloc[:,0:10]\n",
    "m,n,p = linalg.svd(X_23)\n",
    "print(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.971401369688\n",
      "[[ 0.51029346  0.83047315  0.51029346  0.51029346  0.51029346  0.51029346\n",
      "   0.51029346]\n",
      " [ 0.96033906  0.96666427  0.96510257  0.86856999  0.60219054  0.51190636\n",
      "   0.51029346]\n",
      " [ 0.96351366  0.97140137  0.98251328  0.97147657  0.94302835  0.88447021\n",
      "   0.5515417 ]\n",
      " [ 0.96507616  0.97140137  0.98568788  0.98576309  0.97628968  0.95729087\n",
      "   0.79262833]\n",
      " [ 0.96348886  0.97140137  0.98732559  0.98737599  0.97470238  0.95729087\n",
      "   0.80371544]\n",
      " [ 0.96031426  0.97460157  0.98573829  0.98737599  0.97470238  0.95729087\n",
      "   0.80371544]\n",
      " [ 0.96661386  0.97780178  0.98415099  0.98737599  0.97470238  0.95729087\n",
      "   0.80371544]]\n",
      "0.987375992063\n"
     ]
    }
   ],
   "source": [
    "#Taking the first 10 features\n",
    "clf = svm.SVC()\n",
    "# clf.fit(X_23,Y_23)\n",
    "score = cross_validation.cross_val_score(clf, X_23[:,0:10],Y_23, cv=10, n_jobs=4).mean()\n",
    "print(score)\n",
    "C = [0.01,0.1,0.5,1,1.5,2,10]\n",
    "G = [0.01,0.1,0.3,0.5,0.7,0.9,2]\n",
    "scores = np.zeros([7,7])\n",
    "for i in range(0,7):\n",
    "    for j in range(0,7):\n",
    "        clf = svm.SVC(C = C[i], gamma=G[j])\n",
    "        score = cross_validation.cross_val_score(clf, X_23[:,0:10],Y_23, cv=10, n_jobs=4).mean()\n",
    "        scores[i,j] = score\n",
    "print(scores)\n",
    "print(np.amax(scores))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.977826580901\n",
      "[[ 0.51029346  0.51029346  0.51029346  0.51029346  0.51029346  0.51029346\n",
      "   0.51029346]\n",
      " [ 0.95880136  0.8859807   0.51029346  0.51029346  0.51029346  0.51029346\n",
      "   0.51029346]\n",
      " [ 0.97301267  0.97462638  0.57056532  0.53566708  0.52140617  0.51029346\n",
      "   0.51029346]\n",
      " [ 0.97303827  0.97777618  0.86867079  0.67369832  0.56424091  0.54045459\n",
      "   0.53566708]\n",
      " [ 0.97462558  0.97618888  0.8781698   0.69912234  0.58331493  0.553129\n",
      "   0.53566708]\n",
      " [ 0.97306388  0.97618888  0.8781698   0.69912234  0.58331493  0.553129\n",
      "   0.53566708]\n",
      " [ 0.97938908  0.97618888  0.8781698   0.69912234  0.58331493  0.553129\n",
      "   0.53566708]]\n",
      "0.979389080901\n"
     ]
    }
   ],
   "source": [
    "#for the entire 25 features\n",
    "\n",
    "\n",
    "clf = svm.SVC()\n",
    "# clf.fit(X_23,Y_23)\n",
    "score = cross_validation.cross_val_score(clf, X_23,Y_23, cv=10, n_jobs=4).mean()\n",
    "print(score)\n",
    "C = [0.01,0.1,0.5,1,1.5,2,10]\n",
    "G = [0.01,0.1,0.3,0.5,0.7,0.9,2]\n",
    "scores = np.zeros([7,7])\n",
    "for i in range(0,7):\n",
    "    for j in range(0,7):\n",
    "        clf = svm.SVC(C = C[i], gamma=G[j])\n",
    "        score = cross_validation.cross_val_score(clf, X_23,Y_23, cv=10, n_jobs=4).mean()\n",
    "        scores[i,j] = score\n",
    "print(scores)\n",
    "print(np.amax(scores))       \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Observation\n",
    "* When we use 10 features , we can see that we get a better accuracy than when we use the entire 25 features (0.9873 vs 0.9793). This might be due to the reasons:\n",
    "    * When we distinguish among only two classes(in this case 2 & 3), the most of the variation might have been captured in the first 10 dimensions only. \n",
    "    * Also, the other dimensions might be corresponding to the noise in data, and in this way , we might be reducing the load on the algorithm to discover those dimensions which contribute to noise.\n",
    "\n",
    "* We can see that using the default settings , we get a accuracy of 0.979. Using a little bit of tuning we saw that we get the best value which is better than the default settings.\n",
    "\n",
    "### In the next section, we vary the kernel and do the same optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.5102934587813619, 0.5102934587813619, 0.5102934587813619, 0.95570276497695839, 0.96835157450076781, 0.97785138248847936, 0.97460077444956461, 0.5102934587813619, 0.5102934587813619, 0.95887736815156155, 0.97785138248847936, 0.98095078084997456, 0.98092517921146949, 0.97460077444956461, 0.5102934587813619, 0.54509168586789547, 0.97311427931387606, 0.98095078084997456, 0.98092517921146949, 0.97618807603686641, 0.97460077444956461, 0.5102934587813619, 0.95731486815156153, 0.98095078084997456, 0.97933787762416791, 0.97460077444956461, 0.97460077444956461, 0.97460077444956461, 0.5102934587813619, 0.97785138248847936, 0.97933787762416791, 0.97460077444956461, 0.97460077444956461, 0.97460077444956461, 0.97460077444956461]\n",
      "11\n",
      "0.98095078085\n"
     ]
    }
   ],
   "source": [
    "#Using some custom settings to optimize and using the entire 25 features\n",
    "C = [0.1,0.5,1,3,10]\n",
    "G = [0.001,0.01,0.02,0.03,0.04,0.05,1]\n",
    "scores = []\n",
    "for c in C:\n",
    "    for g in G:\n",
    "        clf = svm.SVC(C = c, gamma=g,kernel='poly')\n",
    "        score = cross_validation.cross_val_score(clf, X_23,Y_23, cv=10, n_jobs=4).mean()\n",
    "        scores.append(score)\n",
    "print(scores)\n",
    "print(scores.index(max(scores)))\n",
    "print(scores[scores.index(max(scores))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.5102934587813619, 0.5102934587813619, 0.5102934587813619, 0.51669386840757814, 0.86532098054275475, 0.95560115847414229, 0.96832757296466965, 0.5102934587813619, 0.5102934587813619, 0.62282386072708662, 0.95560115847414229, 0.96038866487455188, 0.95567716333845354, 0.95235295058883762, 0.5102934587813619, 0.5102934587813619, 0.91443052355350751, 0.9619511648745519, 0.95567716333845354, 0.96510176651305668, 0.95235295058883762, 0.5102934587813619, 0.52773057475678442, 0.96195116487455201, 0.95882696492575514, 0.96668906810035826, 0.97150217613927281, 0.95235295058883762, 0.5102934587813619, 0.94453965053763445, 0.95882696492575514, 0.96822596646185344, 0.97152697772657448, 0.96832757296466965, 0.95235295058883762]\n",
      "32\n",
      "0.971526977727\n"
     ]
    }
   ],
   "source": [
    "#Using some custom settings to optimize and using the first 10 features\n",
    "\n",
    "C = [0.1,0.5,1,3,10]\n",
    "G = [0.001,0.01,0.02,0.03,0.04,0.05,1]\n",
    "scores = []\n",
    "for c in C:\n",
    "    for g in G:\n",
    "        clf = svm.SVC(C = c, gamma=g,kernel='poly')\n",
    "        score = cross_validation.cross_val_score(clf, X_23[:,0:10],Y_23, cv=10, n_jobs=4).mean()\n",
    "        scores.append(score)\n",
    "print(scores)\n",
    "print(scores.index(max(scores)))\n",
    "print(scores[scores.index(max(scores))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Observation\n",
    "* We see that the best result is obtained when C = 0.5 and gamma = 0.04. This is a maxima and we decrease our accuracy by moving in any direction. The best result is 0.98095\n",
    "* Also, when we use first 10 features, we get a worse performance than using 25 features (for a polynolial kernel)\n",
    "#### Result: polynomial kernel gives a better performance than rbf kernel, but for first 10 dimensions, rbf kernel is better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.952376952124936, 0.96192636328725034, 0.96351366487455192, 0.96510176651305668, 0.96033906169994876, 0.95872615847414233, 0.9555259536610341, 0.95716365847414231, 0.95557635688684073]\n",
      "3\n",
      "0.965101766513\n"
     ]
    }
   ],
   "source": [
    "#using the first 10 features\n",
    "C = [0.001,0.003,0.01,0.03,0.1,0.5,1,3,10]\n",
    "scores = []\n",
    "for c in C:\n",
    "    clf = svm.SVC(C = c,kernel='linear')\n",
    "    score = cross_validation.cross_val_score(clf, X_23[:,0:10],Y_23, cv=10, n_jobs=4).mean()\n",
    "    scores.append(score)\n",
    "print(scores)\n",
    "print(scores.index(max(scores)))\n",
    "print(scores[scores.index(max(scores))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.9508640552995391, 0.95721406169994872, 0.96822596646185366, 0.96668906810035826, 0.97142697132616473, 0.96029025857654882, 0.95557795698924719, 0.95714045698924721, 0.95719086021505362]\n",
      "4\n",
      "0.971426971326\n"
     ]
    }
   ],
   "source": [
    "C = [0.001,0.003,0.01,0.03,0.1,0.5,1,3,10]\n",
    "scores = []\n",
    "for c in C:\n",
    "    clf = svm.SVC(C = c,kernel='linear')\n",
    "    score = cross_validation.cross_val_score(clf, X_23,Y_23, cv=10, n_jobs=4).mean()\n",
    "    scores.append(score)\n",
    "print(scores)\n",
    "print(scores.index(max(scores)))\n",
    "print(scores[scores.index(max(scores))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Observation\n",
    "* We see that the best performance we can extract by using a linear kernel is 0.9714, which is worse than both polynomial kernel and rbf kernel. \n",
    "* We get better performance when we use the entire 25 features\n",
    "* Also from here we get an intuition that maybe higher order polynomials may perform better. We check that in the next section\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.51029346  0.51029346  0.51029346  0.51029346  0.86839798  0.91606903\n",
      "   0.96515217]\n",
      " [ 0.51029346  0.51029346  0.51029346  0.91599382  0.93032994  0.95255376\n",
      "   0.96515217]\n",
      " [ 0.51029346  0.51029346  0.90961902  0.94620376  0.96361527  0.97467678\n",
      "   0.96515217]\n",
      " [ 0.51029346  0.51029346  0.91765553  0.95572837  0.97308948  0.96986367\n",
      "   0.96515217]\n",
      " [ 0.51029346  0.51029346  0.95094086  0.97308948  0.97147657  0.96512737\n",
      "   0.96515217]\n",
      " [ 0.51029346  0.94781506  0.96674027  0.97147657  0.96671467  0.96515217\n",
      "   0.96515217]\n",
      " [ 0.51029346  0.91455613  0.97150218  0.96668907  0.96671467  0.96515217\n",
      "   0.96515217]]\n",
      "0.974676779314\n"
     ]
    }
   ],
   "source": [
    "# Increase the degree of the polynomial kernel\n",
    "#Using some custom settings to optimize further\n",
    "C = [0.03,0.1,0.5,1,3,10,15]\n",
    "G = [0.001,0.01,0.02,0.03,0.04,0.05,1]\n",
    "scores = np.zeros([7,7])\n",
    "i = 0\n",
    "for i in range(0,7):\n",
    "    for j in range(0,7):\n",
    "        clf = svm.SVC(C = C[i], gamma=G[j],kernel='poly', degree=4)\n",
    "        score = cross_validation.cross_val_score(clf, X_23,Y_23, cv=10, n_jobs=4).mean()\n",
    "#         score = clf.fit(X_23,Y_23).score(X_23,Y_23)\n",
    "        scores[i,j] = score\n",
    "print(scores)\n",
    "print(np.amax(scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Cvalues = np.repeat(C,7)\n",
    "Gvalues = np.tile(G,7)\n",
    "\n",
    "plt.contourf(np.log(Cvalues.reshape(len(C), len(C))), np.log(Gvalues.reshape(len(C), len(C))),\n",
    "             scores)\n",
    "plt.colorbar()\n",
    "plt.title('Train Accuracy')\n",
    "plt.xlabel('log(C)')\n",
    "plt.ylabel('log(gamma)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Observation\n",
    "\n",
    "We see that the polynomial kernel decreases in accuracy when we increase the degree of the kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 46.72695527  41.83510941  38.97715922  37.75855601  33.62344887\n",
      "  32.10558063  30.58285761  28.71829002  26.46569952  25.93141986\n",
      "  22.93955859  22.46139355  21.57496245  20.59900655  19.83863977\n",
      "  17.50594701  15.76717351  14.67584491  13.67052227  12.91923096\n",
      "  12.53860723   9.85948918   9.16887595   8.15218324   6.19211096]\n"
     ]
    }
   ],
   "source": [
    "# Now do the same thing for two other classes\n",
    "# This time take the classes to be 5 and 6\n",
    "\n",
    "B = np.zeros(Y_train.shape[0],dtype=bool)\n",
    "for i in range(0,Y_train.shape[0]):\n",
    "    if(Y_train[i] == 5 or Y_train[i] == 6):\n",
    "        B[i] = True\n",
    "\n",
    "X_56 = X_train[B]\n",
    "Y_56 = Y_train[B]\n",
    "m,n,p = linalg.svd(X_56)\n",
    "print(n)\n",
    "\n",
    "\n",
    "\n",
    "# X_23.iloc[:,0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.988218390805\n",
      "[[ 0.97482759  0.97994253  0.744549    0.65518118  0.54802455  0.53618839\n",
      "   0.52940873]\n",
      " [ 0.97477011  0.98994253  0.88408533  0.79994935  0.79653224  0.64064095\n",
      "   0.52432398]\n",
      " [ 0.97982759  0.98994253  0.890752    0.82006429  0.81009449  0.65745081\n",
      "   0.5260189 ]\n",
      " [ 0.97816092  0.9916092   0.890752    0.82006429  0.81009449  0.65745081\n",
      "   0.5260189 ]\n",
      " [ 0.98321839  0.9916092   0.890752    0.82006429  0.81009449  0.65745081\n",
      "   0.5260189 ]\n",
      " [ 0.98488506  0.9916092   0.890752    0.82006429  0.81009449  0.65745081\n",
      "   0.5260189 ]\n",
      " [ 0.98488506  0.9916092   0.890752    0.82006429  0.81009449  0.65745081\n",
      "   0.5260189 ]]\n",
      "0.991609195402\n"
     ]
    }
   ],
   "source": [
    "# Using the entire 25 features\n",
    "\n",
    "clf = svm.SVC()\n",
    "score = cross_validation.cross_val_score(clf, X_56,Y_56, cv=10, n_jobs=4).mean()\n",
    "print(score)\n",
    "C = [0.5,1,1.5,2,10,20,100]\n",
    "G = [0.01,0.1,0.3,0.5,0.7,0.9,2]\n",
    "scores = np.zeros([7,7])\n",
    "for i in range(0,7):\n",
    "    for j in range(0,7):\n",
    "        clf = svm.SVC(C = C[i], gamma=G[j])\n",
    "        score = cross_validation.cross_val_score(clf, X_56,Y_56, cv=10, n_jobs=4).mean()\n",
    "#         score = clf.fit(X_56,Y_56).score(X_56,Y_56)\n",
    "        scores[i,j] = score\n",
    "print(scores)\n",
    "print(np.amax(scores))  \n",
    "Cvalues = np.repeat(C,7)\n",
    "Gvalues = np.tile(G,7)\n",
    "\n",
    "# plt.contourf(np.log(Cvalues.reshape(len(C), len(C))), np.log(Gvalues.reshape(len(C), len(C))),\n",
    "#              scores)\n",
    "# plt.colorbar()\n",
    "# plt.title('Train Accuracy')\n",
    "# plt.xlabel('log(C)')\n",
    "# plt.ylabel('log(gamma)')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.981551724138\n",
      "[[ 0.93804598  0.97643678  0.97821839  0.97321839  0.94790376  0.92773135\n",
      "   0.77118741]\n",
      " [ 0.93801773  0.98155172  0.97821839  0.97821839  0.96974187  0.95793201\n",
      "   0.88054841]\n",
      " [ 0.9496844   0.98155172  0.97988506  0.97821839  0.9680752   0.95959868\n",
      "   0.88721508]\n",
      " [ 0.94801773  0.98321839  0.97988506  0.97655172  0.9680752   0.95959868\n",
      "   0.88721508]\n",
      " [ 0.966466    0.97816092  0.97482759  0.97655172  0.9680752   0.95959868\n",
      "   0.88721508]\n",
      " [ 0.97152348  0.97649425  0.97482759  0.97655172  0.9680752   0.95959868\n",
      "   0.88721508]\n",
      " [ 0.97821839  0.971466    0.97482759  0.97655172  0.9680752   0.95959868\n",
      "   0.88721508]]\n",
      "0.983218390805\n"
     ]
    }
   ],
   "source": [
    "clf = svm.SVC()\n",
    "score = cross_validation.cross_val_score(clf, X_56[:,0:10],Y_56, cv=10, n_jobs=4).mean()\n",
    "print(score)\n",
    "C = [0.5,1,1.5,2,10,20,100]\n",
    "G = [0.01,0.1,0.3,0.5,0.7,0.9,2]\n",
    "scores = np.zeros([7,7])\n",
    "for i in range(0,7):\n",
    "    for j in range(0,7):\n",
    "        clf = svm.SVC(C = C[i], gamma=G[j])\n",
    "        score = cross_validation.cross_val_score(clf, X_56[:,0:10],Y_56, cv=10, n_jobs=4).mean()\n",
    "#         score = clf.fit(X_56,Y_56).score(X_56,Y_56)\n",
    "        scores[i,j] = score\n",
    "print(scores)\n",
    "print(np.amax(scores)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Observation\n",
    "* We can see that using the default settings , we get a accuracy of 0.9882. Using a little bit of tuning we saw that we get the best value of 0.9916. We see that it is in somewhere in the middle of the matrix. So it is the optimal and we have obtained the optimal value.\n",
    "* We see that reducing the number of features reduces the accuracy of the model. <b>This is in contrast to the  2,3 case.</b> The reasons for this might be that the first 10 dimensions might not capture the variation present in the data set, and so we need further features.\n",
    "<img src=\"56_1.png\" style=\"max-width:100%; width: 50%\"><img src=\"56_5.png\" style=\"max-width:100%; width: 50%\">\n",
    "### In the next section, we vary the kernel and do the same optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.53288428  0.98319014  0.98319014  0.98319014  0.98319014  0.98319014\n",
      "   0.98319014]\n",
      " [ 0.68052796  0.98319014  0.98319014  0.98319014  0.98319014  0.98319014\n",
      "   0.98319014]\n",
      " [ 0.872331    0.98319014  0.98319014  0.98319014  0.98319014  0.98319014\n",
      "   0.98319014]\n",
      " [ 0.92936976  0.98319014  0.98319014  0.98319014  0.98319014  0.98319014\n",
      "   0.98319014]\n",
      " [ 0.97319014  0.98319014  0.98319014  0.98319014  0.98319014  0.98319014\n",
      "   0.98319014]\n",
      " [ 0.98319014  0.98319014  0.98319014  0.98319014  0.98319014  0.98319014\n",
      "   0.98319014]\n",
      " [ 0.98488603  0.98319014  0.98319014  0.98319014  0.98319014  0.98319014\n",
      "   0.98319014]]\n",
      "0.98488603156\n"
     ]
    }
   ],
   "source": [
    "C = [0.5,1,1.5,2,10,20,100]\n",
    "G = [0.01,0.1,0.3,0.5,0.7,0.9,2]\n",
    "scores = np.zeros([7,7])\n",
    "for i in range(0,7):\n",
    "    for j in range(0,7):\n",
    "        clf = svm.SVC(C = C[i], gamma=G[j], kernel='poly')\n",
    "        score = cross_validation.cross_val_score(clf, X_56,Y_56, cv=10, n_jobs=4).mean()\n",
    "#         score = clf.fit(X_56,Y_56).score(X_56,Y_56)\n",
    "        scores[i,j] = score\n",
    "print(scores)\n",
    "print(np.amax(scores)) \n",
    "# Cvalues = np.repeat(C,7)\n",
    "# Gvalues = np.tile(G,7)\n",
    "\n",
    "# plt.contourf(np.log(Cvalues.reshape(len(C), len(C))), np.log(Gvalues.reshape(len(C), len(C))),\n",
    "#              scores)\n",
    "# plt.colorbar()\n",
    "# plt.title('Train Accuracy')\n",
    "# plt.xlabel('log(C)')\n",
    "# plt.ylabel('log(gamma)')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.51101695  0.95985681  0.97816092  0.95977109  0.96310442  0.96310442\n",
      "   0.96310442]\n",
      " [ 0.51101695  0.96824761  0.96977011  0.96310442  0.96310442  0.96310442\n",
      "   0.96310442]\n",
      " [ 0.51101695  0.96485681  0.96138028  0.96310442  0.96310442  0.96310442\n",
      "   0.96310442]\n",
      " [ 0.51101695  0.96494253  0.95977109  0.96310442  0.96310442  0.96310442\n",
      "   0.96310442]\n",
      " [ 0.7360715   0.97149425  0.96310442  0.96310442  0.96310442  0.96310442\n",
      "   0.96310442]\n",
      " [ 0.83365381  0.97310345  0.96310442  0.96310442  0.96310442  0.96310442\n",
      "   0.96310442]\n",
      " [ 0.90945646  0.96143776  0.96310442  0.96310442  0.96310442  0.96310442\n",
      "   0.96310442]]\n",
      "0.97816091954\n"
     ]
    }
   ],
   "source": [
    "C = [0.5,1,1.5,2,10,20,100]\n",
    "G = [0.01,0.1,0.3,0.5,0.7,0.9,2]\n",
    "scores = np.zeros([7,7])\n",
    "for i in range(0,7):\n",
    "    for j in range(0,7):\n",
    "        clf = svm.SVC(C = C[i], gamma=G[j], kernel='poly')\n",
    "        score = cross_validation.cross_val_score(clf, X_56[:,0:10],Y_56, cv=10, n_jobs=4).mean()\n",
    "#         score = clf.fit(X_56,Y_56).score(X_56,Y_56)\n",
    "        scores[i,j] = score\n",
    "print(scores)\n",
    "print(np.amax(scores)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Observations\n",
    "* Using all of the features gives a better performance than using just the first 10 features.\n",
    "* We see that in this case , the best performance obtained is 0.984 which is better than the default settings for the kernel.\n",
    "<img src=\"56_3.png\" style=\"max-width:100%; width: 50%\">\n",
    "<img src=\"56_7.png\" style=\"max-width:100%; width: 50%\">\n",
    "### Result : For this case, we see that polynomial kernel gives a worse performance than rbf kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.95635106]\n",
      " [ 0.9530752 ]\n",
      " [ 0.9546844 ]\n",
      " [ 0.95635106]\n",
      " [ 0.96137931]\n",
      " [ 0.96143678]\n",
      " [ 0.95477011]]\n",
      "0.961436781609\n"
     ]
    }
   ],
   "source": [
    "#using the linear kernel\n",
    "C = [0.5,1,1.5,2,10,20,100]\n",
    "G = [0.01,0.1,0.3,0.5,0.7,0.9,2]\n",
    "scores = np.zeros([7,1])\n",
    "for i in range(0,7):\n",
    "    clf = svm.SVC(C = C[i], kernel='linear')\n",
    "    score = cross_validation.cross_val_score(clf, X_56,Y_56, cv=10, n_jobs=4).mean()\n",
    "    scores[i] = score\n",
    "print(scores)\n",
    "print(np.amax(scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Observation\n",
    "* We see that the best performance we can extract by using a linear kernel is 0.9614, which is worse than both polynomial kernel and rbf kernel.\n",
    "* Also from here we get an intuition that maybe higher order polynomials may perform better. We check that in the next section\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.50254237  0.81336743  0.96965615  0.98155172  0.97982759  0.97982759\n",
      "   0.97982759]\n",
      " [ 0.50254237  0.9645412   0.97985681  0.9866092   0.97982759  0.97982759\n",
      "   0.97982759]\n",
      " [ 0.50254237  0.97471362  0.97982759  0.98321839  0.97982759  0.97982759\n",
      "   0.97982759]\n",
      " [ 0.58185272  0.97132281  0.98321839  0.98149425  0.97982759  0.97982759\n",
      "   0.97982759]\n",
      " [ 0.86882525  0.97301773  0.97982759  0.97982759  0.97982759  0.97982759\n",
      "   0.97982759]\n",
      " [ 0.96959868  0.98155172  0.97982759  0.97982759  0.97982759  0.97982759\n",
      "   0.97982759]\n",
      " [ 0.96959868  0.98149425  0.97982759  0.97982759  0.97982759  0.97982759\n",
      "   0.97982759]]\n",
      "0.986609195402\n"
     ]
    }
   ],
   "source": [
    "#Changing the degree of the polynomial kernel and using all the features\n",
    "C = [0.5,1,1.5,2,10,20,100]\n",
    "G = [0.01,0.02,0.04,0.05,0.1,0.3,1]\n",
    "scores = np.zeros([7,7])\n",
    "for i in range(0,7):\n",
    "    for j in range(0,7):\n",
    "        clf = svm.SVC(C = C[i], gamma=G[j], kernel='poly', degree=4)\n",
    "        score = cross_validation.cross_val_score(clf, X_56,Y_56, cv=10, n_jobs=4).mean()\n",
    "#         score = clf.fit(X_56,Y_56).score(X_56,Y_56)\n",
    "        scores[i,j] = score\n",
    "print(scores)\n",
    "print(np.amax(scores))\n",
    "\n",
    "# plt.contourf(np.log(Cvalues.reshape(len(C), len(C))), np.log(Gvalues.reshape(len(C), len(C))),\n",
    "#              scores)\n",
    "# plt.colorbar()\n",
    "# plt.title('Train Accuracy')\n",
    "# plt.xlabel('log(C)')\n",
    "# plt.ylabel('log(gamma)')\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.5059322   0.5059322   0.6655562   0.73770894  0.92603741  0.94971459\n",
      "   0.94138126]\n",
      " [ 0.5059322   0.5059322   0.73604228  0.78471849  0.94465712  0.93620884\n",
      "   0.94138126]\n",
      " [ 0.5059322   0.53116209  0.7545188   0.81845315  0.95138126  0.94129456\n",
      "   0.94138126]\n",
      " [ 0.5059322   0.69058738  0.77129944  0.83350964  0.95138126  0.9430187\n",
      "   0.94138126]\n",
      " [ 0.5059322   0.68736606  0.88575005  0.93606565  0.95146698  0.94138126\n",
      "   0.94138126]\n",
      " [ 0.50759887  0.741128    0.92770407  0.94804793  0.94638126  0.94138126\n",
      "   0.94138126]\n",
      " [ 0.63538477  0.85710111  0.95143873  0.95480031  0.93959965  0.94138126\n",
      "   0.94138126]]\n",
      "0.954800311709\n"
     ]
    }
   ],
   "source": [
    "#Changing the degree of the polynomial kernel and using the first 10 features\n",
    "C = [0.5,1,1.5,2,10,20,100]\n",
    "G = [0.01,0.02,0.04,0.05,0.1,0.3,1]\n",
    "scores = np.zeros([7,7])\n",
    "for i in range(0,7):\n",
    "    for j in range(0,7):\n",
    "        clf = svm.SVC(C = C[i], gamma=G[j], kernel='poly', degree=4)\n",
    "        score = cross_validation.cross_val_score(clf, X_56[:,0:10],Y_56, cv=10, n_jobs=4).mean()\n",
    "#         score = clf.fit(X_56,Y_56).score(X_56,Y_56)\n",
    "        scores[i,j] = score\n",
    "print(scores)\n",
    "print(np.amax(scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Observation\n",
    "\n",
    "* We see that by increasing the degree of the polynomial, we have increased the (best) accuracy of the model, so we can conclude that increasing the degree is increasing the accuracy in this case. We check for higher degree in the next section.\n",
    "* Also , using all the features give better performance than first 10 features.\n",
    "<img src=\"56_8.png\" style=\"max-width:100%; width: 50%\">\n",
    "<img src=\"56_9.png\" style=\"max-width:100%; width: 50%\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.97307519968829137]\n",
      "[[ 0.50084746  0.58147769  0.96293201  0.97979934  0.98324761  0.98324761\n",
      "   0.98324761]\n",
      " [ 0.50084746  0.77641535  0.9730752   0.98155172  0.98324761  0.98324761\n",
      "   0.98324761]\n",
      " [ 0.50084746  0.8588535   0.97813267  0.98152348  0.98324761  0.98324761\n",
      "   0.98324761]\n",
      " [ 0.50084746  0.88899669  0.98319014  0.98152348  0.98324761  0.98324761\n",
      "   0.98324761]\n",
      " [ 0.52101987  0.95275959  0.97985681  0.98324761  0.98324761  0.98324761\n",
      "   0.98324761]\n",
      " [ 0.62354568  0.96798948  0.98324761  0.98324761  0.98324761  0.98324761\n",
      "   0.98324761]\n",
      " [ 0.92086402  0.98155172  0.98324761  0.98324761  0.98324761  0.98324761\n",
      "   0.98324761]]\n",
      "0.983247613481\n"
     ]
    }
   ],
   "source": [
    "#Changing the degree of the polynomial kernel\n",
    "scores = []\n",
    "\n",
    "C = [0.5,1,1.5,2,10,20,100]\n",
    "G = [0.01,0.02,0.04,0.05,0.1,0.3,1]\n",
    "scores = np.zeros([7,7])\n",
    "for i in range(0,7):\n",
    "    for j in range(0,7):\n",
    "        clf = svm.SVC(C = C[i], gamma=G[j], kernel='poly', degree=5)\n",
    "        score = cross_validation.cross_val_score(clf, X_56,Y_56, cv=10, n_jobs=4).mean()\n",
    "        #         score = clf.fit(X_23,Y_23).score(X_23,Y_23)\n",
    "        scores[i,j] = score\n",
    "print(scores)\n",
    "print(np.amax(scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Observations\n",
    "We see that for degree 5, the accuracy is less than degree 4. So we can conclude that the best accuracy is obtained for polynomial kernel of degree 4. \n",
    "### Now we repeat the same analysis for classes 1 and 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.987499236874\n",
      "[[ 0.93730408  0.96394984  0.98589342  0.99373041  0.98119122  0.97021944\n",
      "   0.96394984]\n",
      " [ 0.95297806  0.97335423  0.98902821  1.          1.          1.          1.        ]\n",
      " [ 0.95611285  0.97492163  0.98902821  1.          1.          1.          1.        ]\n",
      " [ 0.96081505  0.97648903  0.98902821  1.          1.          1.          1.        ]\n",
      " [ 0.97335423  0.98746082  0.99373041  1.          1.          1.          1.        ]\n",
      " [ 0.97805643  0.98746082  0.9968652   1.          1.          1.          1.        ]\n",
      " [ 0.98589342  0.99529781  1.          1.          1.          1.          1.        ]]\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "B = np.zeros(Y_train.shape[0],dtype=bool)\n",
    "for i in range(0,Y_train.shape[0]):\n",
    "    if(Y_train[i] == 1 or Y_train[i] == 8):\n",
    "        B[i] = True\n",
    "\n",
    "X_18 = X_train[B]\n",
    "Y_18 = Y_train[B]\n",
    "# X_23.iloc[:,0:10]\n",
    "clf = svm.SVC()\n",
    "# clf.fit(X_23,Y_23)\n",
    "score = cross_validation.cross_val_score(clf, X_18,Y_18, cv=10, n_jobs=4).mean()\n",
    "print(score)\n",
    "C = [0.5,1,1.5,2,10,20,100]\n",
    "G = [0.001,0.003,0.01,0.1,0.3,0.5,0.7]\n",
    "scores = np.zeros([7,7])\n",
    "for i in range(0,7):\n",
    "    for j in range(0,7):\n",
    "        clf = svm.SVC(C = C[i], gamma=G[j])\n",
    "#         score = cross_validation.cross_val_score(clf, X_18,Y_18, cv=10, n_jobs=4).mean()\n",
    "        score = clf.fit(X_18,Y_18).score(X_18,Y_18)\n",
    "        scores[i,j] = score\n",
    "print(scores)\n",
    "print(np.amax(scores))  \n",
    "Cvalues = np.repeat(C,7)\n",
    "Gvalues = np.tile(G,7)\n",
    "\n",
    "plt.contourf(np.log(Cvalues.reshape(len(C), len(C))), np.log(Gvalues.reshape(len(C), len(C))),\n",
    "             scores)\n",
    "plt.colorbar()\n",
    "plt.title('Train Accuracy')\n",
    "plt.xlabel('log(C)')\n",
    "plt.ylabel('log(gamma)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Observations\n",
    "The best accuracy obtained is 0.9874 using the rbf kernel for this case. \n",
    "<img src=\"18_1.png\" style=\"max-width:100%; width: 50%\">\n",
    "<img src=\"18_2.png\" style=\"max-width:100%; width: 50%\">\n",
    " ### Now we change the kernel type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.98898809523809528]\n",
      "[[ 0.52820055  0.98276213  0.9889881   0.52820055  0.9874256   0.9889881\n",
      "   0.9889881 ]\n",
      " [ 0.52820055  0.98432463  0.9889881   0.52820055  0.9889881   0.9889881\n",
      "   0.9889881 ]\n",
      " [ 0.52820055  0.98588713  0.9889881   0.53916361  0.9889881   0.9889881\n",
      "   0.9889881 ]\n",
      " [ 0.53757631  0.98588713  0.9889881   0.59236607  0.9889881   0.9889881\n",
      "   0.9889881 ]\n",
      " [ 0.97028465  0.9889881   0.9889881   0.97653541  0.9889881   0.9889881\n",
      "   0.9889881 ]\n",
      " [ 0.98432463  0.9874256   0.9889881   0.98276213  0.9889881   0.9889881\n",
      "   0.9889881 ]\n",
      " [ 0.9889881   0.9889881   0.9889881   0.9889881   0.9889881   0.9889881\n",
      "   0.9889881 ]]\n",
      "0.988988095238\n"
     ]
    }
   ],
   "source": [
    "#using the default settings for the polynomial kernel\n",
    "scores = []\n",
    "\n",
    "clf = svm.SVC(kernel='poly')\n",
    "score = cross_validation.cross_val_score(clf, X_18,Y_18, cv=10, n_jobs=4).mean()\n",
    "scores.append(score)\n",
    "print(scores)\n",
    "\n",
    "C = [0.5,1,1.5,2,10,20,100]\n",
    "G = [0.009,0.03,0.05,0.01,0.1,0.3,0.5]\n",
    "scores = np.zeros([7,7])\n",
    "for i in range(0,7):\n",
    "    for j in range(0,7):\n",
    "        clf = svm.SVC(C = C[i], gamma=G[j], kernel='poly')\n",
    "        score = cross_validation.cross_val_score(clf, X_18,Y_18, cv=10, n_jobs=4).mean()\n",
    "#         score = clf.fit(X_18,Y_18).score(X_18,Y_18)\n",
    "        scores[i,j] = score\n",
    "print(scores)\n",
    "print(np.amax(scores)) \n",
    "Cvalues = np.repeat(C,7)\n",
    "Gvalues = np.tile(G,7)\n",
    "\n",
    "plt.contourf(np.log(Cvalues.reshape(len(C), len(C))), np.log(Gvalues.reshape(len(C), len(C))),\n",
    "             scores)\n",
    "plt.colorbar()\n",
    "plt.title('Train Accuracy')\n",
    "plt.xlabel('log(C)')\n",
    "plt.ylabel('log(gamma)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Observation\n",
    "The best accuracy obtained is 0.9889 which is better than rbf kernel.\n",
    "\n",
    "### Result: Polynomial kernel is better than rbf kernel for this case.\n",
    "\n",
    "Now we check for linear kernel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.96872215]\n",
      " [ 0.97028541]\n",
      " [ 0.97499771]\n",
      " [ 0.97026061]\n",
      " [ 0.97026061]\n",
      " [ 0.97028465]\n",
      " [ 0.96869811]]\n",
      "0.974997710623\n"
     ]
    }
   ],
   "source": [
    "#using the linear kernel\n",
    "C = [0.1,0.3,0.5,1,1.5,2,10]\n",
    "\n",
    "scores = np.zeros([7,1])\n",
    "for i in range(0,7):\n",
    "    clf = svm.SVC(C = C[i], kernel='linear')\n",
    "    score = cross_validation.cross_val_score(clf, X_18,Y_18, cv=10, n_jobs=4).mean()\n",
    "    scores[i] = score\n",
    "print(scores)\n",
    "print(np.amax(scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Observation\n",
    "Linear kernel performs worse than both rbf and polynomial kernel.\n",
    "Now we vary the degree of the polynomial kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.99213789682539688]\n",
      "[[ 0.52820055  0.52820055  0.52820055  0.85264461  0.9921379   0.52820055\n",
      "   0.98434867]\n",
      " [ 0.52820055  0.52820055  0.52820055  0.97340888  0.9921379   0.52820055\n",
      "   0.98122367]\n",
      " [ 0.52820055  0.52820055  0.52820055  0.98437271  0.98901213  0.52820055\n",
      "   0.98122367]\n",
      " [ 0.52820055  0.52820055  0.52820055  0.99059944  0.98901213  0.52820055\n",
      "   0.98122367]\n",
      " [ 0.52820055  0.52820055  0.52820055  0.9905506   0.98434867  0.55327648\n",
      "   0.98122367]\n",
      " [ 0.52820055  0.52820055  0.59082837  0.98744963  0.98122367  0.67394231\n",
      "   0.98122367]\n",
      " [ 0.52820055  0.52820055  0.95934562  0.98122367  0.98122367  0.97965965\n",
      "   0.98122367]]\n",
      "0.992137896825\n"
     ]
    }
   ],
   "source": [
    "#using the default settings for the polynomial kernel\n",
    "scores = []\n",
    "\n",
    "clf = svm.SVC(kernel='poly', degree = 4)\n",
    "score = cross_validation.cross_val_score(clf, X_18,Y_18, cv=10, n_jobs=4).mean()\n",
    "scores.append(score)\n",
    "print(scores)\n",
    "\n",
    "C = [0.5,1,1.5,2,10,20,100]\n",
    "G = [0.001,0.005,0.009,0.03,0.05,0.01,0.1]\n",
    "scores = np.zeros([7,7])\n",
    "for i in range(0,7):\n",
    "    for j in range(0,7):\n",
    "        clf = svm.SVC(C = C[i], gamma=G[j], kernel='poly', degree=4)\n",
    "        score = cross_validation.cross_val_score(clf, X_18,Y_18, cv=10, n_jobs=4).mean()\n",
    "#         score = clf.fit(X_18,Y_18).score(X_18,Y_18)\n",
    "        scores[i,j] = score\n",
    "print(scores)\n",
    "print(np.amax(scores)) \n",
    "# Cvalues = np.repeat(C,7)\n",
    "# Gvalues = np.tile(G,7)\n",
    "\n",
    "# plt.contourf(np.log(Cvalues.reshape(len(C), len(C))), np.log(Gvalues.reshape(len(C), len(C))),\n",
    "#              scores)\n",
    "# plt.colorbar()\n",
    "# plt.title('Test Accuracy')\n",
    "# plt.xlabel('log(C)')\n",
    "# plt.ylabel('log(gamma)')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Observation\n",
    "We see that with degree 4 , polynomial kernel performs better than degree 3. We now check for degree 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.94047161172161187]\n",
      "[[ 0.52820055  0.52820055  0.52820055  0.63011561  0.96088408  0.52820055\n",
      "   0.98429983]\n",
      " [ 0.52820055  0.52820055  0.52820055  0.71938034  0.96557158  0.52820055\n",
      "   0.98273733]\n",
      " [ 0.52820055  0.52820055  0.52820055  0.80249199  0.97025984  0.52820055\n",
      "   0.98273733]\n",
      " [ 0.52820055  0.52820055  0.52820055  0.83696848  0.97338561  0.52820055\n",
      "   0.98273733]\n",
      " [ 0.52820055  0.52820055  0.52820055  0.96400908  0.98117483  0.52820055\n",
      "   0.98273733]\n",
      " [ 0.52820055  0.52820055  0.52820055  0.97333677  0.98429983  0.52820055\n",
      "   0.98273733]\n",
      " [ 0.52820055  0.52820055  0.56421474  0.98117483  0.98273733  0.60811508\n",
      "   0.98273733]]\n",
      "0.984299832112\n"
     ]
    }
   ],
   "source": [
    "#using the default settings for the polynomial kernel\n",
    "scores = []\n",
    "\n",
    "clf = svm.SVC(kernel='poly', degree = 5)\n",
    "score = cross_validation.cross_val_score(clf, X_18,Y_18, cv=10, n_jobs=4).mean()\n",
    "scores.append(score)\n",
    "print(scores)\n",
    "\n",
    "C = [0.5,1,1.5,2,10,20,100]\n",
    "G = [0.001,0.005,0.009,0.03,0.05,0.01,0.1]\n",
    "scores = np.zeros([7,7])\n",
    "for i in range(0,7):\n",
    "    for j in range(0,7):\n",
    "        clf = svm.SVC(C = C[i], gamma=G[j], kernel='poly', degree=5)\n",
    "        score = cross_validation.cross_val_score(clf, X_18,Y_18, cv=10, n_jobs=4).mean()\n",
    "#         score = clf.fit(X_18,Y_18).score(X_18,Y_18)\n",
    "        scores[i,j] = score\n",
    "print(scores)\n",
    "print(np.amax(scores)) \n",
    "# Cvalues = np.repeat(C,7)\n",
    "# Gvalues = np.tile(G,7)\n",
    "\n",
    "# plt.contourf(np.log(Cvalues.reshape(len(C), len(C))), np.log(Gvalues.reshape(len(C), len(C))),\n",
    "#              scores)\n",
    "# plt.colorbar()\n",
    "# plt.title('Test Accuracy')\n",
    "# plt.xlabel('log(C)')\n",
    "# plt.ylabel('log(gamma)')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Observation\n",
    "We see that for degree 5 , performance decreases. So we can say that optimal performance is obtained at degree 4."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
